# -*- coding: utf-8 -*-
"""research_multi_label_classification_bert_54.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1L2LBUgMp2ExVvGGUYRRS3Ja16VRXYAUH
"""

!pip install transformers[torch] datasets nltk

import json
import pandas as pd
import numpy as np
import torch
import matplotlib.pyplot as plt
import re
from datasets import Dataset
from sklearn.metrics import f1_score
from transformers import (
    TrainingArguments,
    Trainer,
    AutoTokenizer,
    AutoModelForSequenceClassification,
    DataCollatorWithPadding,
    EarlyStoppingCallback,
    MarianMTModel,
    MarianTokenizer
)
from sklearn.model_selection import train_test_split, KFold
from sklearn.preprocessing import MultiLabelBinarizer
from nltk.corpus import wordnet
import nltk
from nltk.tokenize import word_tokenize
import random
import re

nltk.download('wordnet')
nltk.download('punkt')
nltk.download('averaged_perceptron_tagger')

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

classes = ['CE', 'ENV', 'BME', 'PE', 'METAL', 'ME', 'EE', 'CPE', 'OPTIC', 'NANO', 'CHE',
           'MATENG', 'AGRI', 'EDU', 'IE', 'SAFETY', 'MATH', 'MATSCI']

mlb = MultiLabelBinarizer(classes=classes)

with open('data/train_for_student.json', 'r', encoding='utf-8') as f:
    train_for_student = json.load(f)

# You can choose different language pairs. Here we use English to German and back.
en_de_model = MarianMTModel.from_pretrained("Helsinki-NLP/opus-mt-en-de").to(device)
en_de_tokenizer = MarianTokenizer.from_pretrained("Helsinki-NLP/opus-mt-en-de")

de_en_model = MarianMTModel.from_pretrained("Helsinki-NLP/opus-mt-de-en").to(device)
de_en_tokenizer = MarianTokenizer.from_pretrained("Helsinki-NLP/opus-mt-de-en")

def translate(texts, model, tokenizer, language=""):
    formatted_text = [">>{}<< {}".format(language, text) for text in texts]
    tokens = tokenizer(formatted_text, return_tensors="pt", padding=True, truncation=True, max_length=512).to(device)
    translated = model.generate(**tokens)
    translated_texts = tokenizer.batch_decode(translated, skip_special_tokens=True)
    return translated_texts

def backtranslate(texts, source_lang="en", target_lang="de"):
    if target_lang == "de":
        translations = translate(texts, en_de_model, en_de_tokenizer, language=target_lang)
    else:
        translations = translate(texts, de_en_model, de_en_tokenizer, language=target_lang)

    if source_lang == "en":
        back_translations = translate(translations, de_en_model, de_en_tokenizer, language=source_lang)
    else:
        back_translations = translate(translations, en_de_model, en_de_tokenizer, language=source_lang)

    return back_translations

# Improved text preprocessing
def clean_text(text):
    text = re.sub(r'\s+', ' ', text)
    text = re.sub(r'[^\w\s\.\-\,]', '', text)
    return text.lower().strip()

# Enhanced data augmentation techniques
def synonym_replacement(words, n):
    new_words = words.copy()
    random_word_list = list(set([word for word in words if word.isalnum()]))
    random.shuffle(random_word_list)
    num_replaced = 0
    for random_word in random_word_list:
        synonyms = []
        for syn in wordnet.synsets(random_word):
            for l in syn.lemmas():
                synonyms.append(l.name())
        if len(synonyms) >= 1:
            synonym = random.choice(list(set(synonyms)))
            new_words = [synonym if word == random_word else word for word in new_words]
            num_replaced += 1
        if num_replaced >= n:
            break
    return new_words

def random_insertion(words, n):
    new_words = words.copy()
    for _ in range(n):
        add_word(new_words)
    return new_words

def add_word(new_words):
    synonyms = []
    counter = 0
    while len(synonyms) < 1:
        random_word = new_words[random.randint(0, len(new_words)-1)]
        synonyms = get_synonyms(random_word)
        counter += 1
        if counter >= 10:
            return
    random_synonym = synonyms[0]
    random_idx = random.randint(0, len(new_words)-1)
    new_words.insert(random_idx, random_synonym)

def get_synonyms(word):
    synonyms = set()
    for syn in wordnet.synsets(word):
        for l in syn.lemmas():
            synonym = l.name().replace("_", " ").replace("-", " ").lower()
            synonym = "".join([char for char in synonym if char in ' qwertyuiopasdfghjklzxcvbnm'])
            synonyms.add(synonym)
    if word in synonyms:
        synonyms.remove(word)
    return list(synonyms)

def random_swap(words, n):
    new_words = words.copy()
    for _ in range(n):
        new_words = swap_word(new_words)
    return new_words

def swap_word(new_words):
    random_idx_1 = random.randint(0, len(new_words)-1)
    random_idx_2 = random_idx_1
    counter = 0
    while random_idx_2 == random_idx_1:
        random_idx_2 = random.randint(0, len(new_words)-1)
        counter += 1
        if counter > 3:
            return new_words
    new_words[random_idx_1], new_words[random_idx_2] = new_words[random_idx_2], new_words[random_idx_1]
    return new_words

def random_deletion(words, p):
    if len(words) == 1:
        return words
    new_words = []
    for word in words:
        r = random.uniform(0, 1)
        if r > p:
            new_words.append(word)
    if len(new_words) == 0:
        rand_int = random.randint(0, len(words)-1)
        return [words[rand_int]]
    return new_words

def eda(sentence, alpha_sr=0.1, alpha_ri=0.1, alpha_rs=0.1, p_rd=0.1, num_aug=9):
    words = word_tokenize(sentence)
    words = [word for word in words if word is not None]
    num_words = len(words)

    augmented_sentences = []
    num_new_per_technique = int(num_aug/4)+1

    #sr
    for _ in range(num_new_per_technique):
        n_sr = max(1, int(alpha_sr*num_words))
        a_words = synonym_replacement(words, n_sr)
        augmented_sentences.append(' '.join(a_words))

    #ri
    for _ in range(num_new_per_technique):
        n_ri = max(1, int(alpha_ri*num_words))
        a_words = random_insertion(words, n_ri)
        augmented_sentences.append(' '.join(a_words))

    #rs
    for _ in range(num_new_per_technique):
        n_rs = max(1, int(alpha_rs*num_words))
        a_words = random_swap(words, n_rs)
        augmented_sentences.append(' '.join(a_words))

    #rd
    for _ in range(num_new_per_technique):
        a_words = random_deletion(words, p_rd)
        augmented_sentences.append(' '.join(a_words))

    augmented_sentences = [sentence for sentence in augmented_sentences if len(sentence) > 10]
    random.shuffle(augmented_sentences)

    if num_aug >= 1:
        augmented_sentences = augmented_sentences[:num_aug]
    else:
        keep_prob = num_aug / len(augmented_sentences)
        augmented_sentences = [s for s in augmented_sentences if random.uniform(0, 1) < keep_prob]

    augmented_sentences.append(sentence)
    return augmented_sentences

def augment_text(text, title_length, n_aug=9):
    title = text[:title_length]
    body = text[title_length:]
    augmented_bodies = eda(body, num_aug=n_aug)
    return [f"{title}{aug_body}" for aug_body in augmented_bodies]

def process_data(data, augment=True, batch_size=128):
    rows = []
    all_classes = []

    # Process data in batches
    for i in range(0, len(data), batch_size):
        batch = list(data.items())[i:i+batch_size]

        texts = []
        for id, info in batch:
            title = clean_text(info['Title'])
            abstract = clean_text(info['Abstract'])
            text = f"{title} [SEP] {abstract}"
            texts.append(text)

        # Perform backtranslation on the batch
        backtranslated_texts = backtranslate(texts)

        for j, (id, info) in enumerate(batch):
            original_text = texts[j]
            backtranslated_text = backtranslated_texts[j]
            title_length = len(clean_text(info['Title'])) + 6

            if augment and 'Classes' in info:
                # Add original text
                rows.append({"text": original_text})
                all_classes.append(info['Classes'])

                # Add backtranslated text
                rows.append({"text": backtranslated_text})
                all_classes.append(info['Classes'])

                # Apply EDA on both original and backtranslated texts
                original_augmented = augment_text(original_text, title_length, n_aug=9)
                backtranslated_augmented = augment_text(backtranslated_text, title_length, n_aug=4)

                for aug_text in original_augmented + backtranslated_augmented:
                    rows.append({"text": aug_text})
                    all_classes.append(info['Classes'])
            else:
                rows.append({"text": original_text})
                if 'Classes' in info:
                    all_classes.append(info['Classes'])

    df = pd.DataFrame(rows)
    if all_classes:
        labels = mlb.fit_transform(all_classes)
        df['labels'] = [label.tolist() for label in labels]
    return df

train_df = process_data(train_for_student)

print(train_df.shape[0])
train_df.head(5)

train_data, test_data = train_test_split(train_df, test_size=0.4, random_state=42)
val_data, test_data = train_test_split(test_data, test_size=0.5, random_state=42)


hg_train_data = Dataset.from_pandas(train_data)
hg_val_data = Dataset.from_pandas(val_data)
hg_test_data = Dataset.from_pandas(test_data)

print(hg_train_data)

model_name = 'bert-base-uncased'
tokenizer = AutoTokenizer.from_pretrained(model_name, do_lower_case=True)

def tokenize_dataset(data):
    tokenized_data = tokenizer(
        data['text'],
        truncation=True,
        padding='max_length',
        max_length=512,
    )
    return tokenized_data

dataset_train = hg_train_data.map(tokenize_dataset, batched=True)
dataset_val = hg_val_data.map(tokenize_dataset, batched=True)
dataset_test = hg_test_data.map(tokenize_dataset, batched=True)

print(dataset_train[0])

model = AutoModelForSequenceClassification.from_pretrained(
    model_name,
    num_labels=len(classes),
    problem_type='multi_label_classification'
)

# Optimizing thresholds for each label
def optimize_thresholds(predictions, labels, num_points=100):
    best_thresholds = []

    for i in range(predictions.shape[1]):
        best_f1 = 0
        best_threshold = 0.5

        # Get predictions for current class
        class_predictions = predictions[:, i]
        sorted_preds = np.sort(class_predictions)

        # Try different threshold values
        for percentile in range(1, num_points):
            threshold = sorted_preds[int(len(sorted_preds) * (percentile/num_points))]
            binary_predictions = (class_predictions >= threshold).astype(int)
            f1 = f1_score(labels[:, i], binary_predictions)

            if f1 > best_f1:
                best_f1 = f1
                best_threshold = threshold

        best_thresholds.append(best_threshold)

    return best_thresholds

# Custom compute_metrics function
def compute_metrics(p):
    logits = p.predictions
    labels = p.label_ids
    sigmoid = torch.nn.Sigmoid()
    probs = sigmoid(torch.tensor(logits))

    # Optimize thresholds
    thresholds = optimize_thresholds(probs.numpy(), labels)
    predictions = np.array([(probs[:, i] > thresholds[i]).numpy().astype(int) for i in range(len(thresholds))]).T

    # Calculate F1 score
    f1 = f1_score(y_true=labels, y_pred=predictions, average='macro')
    return {
        'macro_f1': f1,
        'thresholds': thresholds
    }

class MultilabelTrainer(Trainer):
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.label_weights = self.compute_class_weights()

    def compute_class_weights(self):
        labels = np.array(self.train_dataset['labels'])
        pos_counts = np.sum(labels, axis=0)
        neg_counts = len(labels) - pos_counts
        weights = np.sqrt(neg_counts / (pos_counts + 1))  # Square root smoothing
        return torch.FloatTensor(weights).to(self.args.device)

    def compute_loss(self, model, inputs, return_outputs=False):
        labels = inputs.pop("labels")
        outputs = model(**inputs)
        logits = outputs.logits

        # Improved focal loss with dynamic alpha
        alpha = 0.75  # Increased focus on positive samples
        gamma = 2.0

        bce_loss = torch.nn.BCEWithLogitsLoss(reduction='none',
                                             pos_weight=self.label_weights)
        base_loss = bce_loss(logits, labels.float())

        probs = torch.sigmoid(logits)
        p_t = probs * labels + (1 - probs) * (1 - labels)
        focal_weight = ((1 - p_t) ** gamma) * alpha

        loss = (focal_weight * base_loss).mean()

        return (loss, outputs) if return_outputs else loss


# Define training arguments
training_args = TrainingArguments(
    output_dir='./results',
    learning_rate=5e-5,
    num_train_epochs=20,
    per_device_train_batch_size=32,
    per_device_eval_batch_size=32,
    gradient_accumulation_steps=2,
    warmup_ratio=0.1,
    weight_decay=0.01,
    logging_steps=50,
    evaluation_strategy="steps",
    eval_steps=100,
    save_strategy="steps",
    save_steps=100,
    load_best_model_at_end=True,
    metric_for_best_model='macro_f1',
    greater_is_better=True,
    fp16=True,
    gradient_checkpointing=True,
    save_total_limit=2,
    seed=42
)

trainer = MultilabelTrainer(
    model=model,
    args=training_args,
    train_dataset=dataset_train,
    eval_dataset=dataset_val,
    compute_metrics=compute_metrics,
    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)],
    data_collator=DataCollatorWithPadding(tokenizer)
)

trainer.train()

optimized_thresholds = [0.40229520928454536, 0.5046116087702355, 0.6885821342468262, 0.3796615387598403, 0.3935696326415061, 0.4158164678568413, 0.45118921086986325, 0.3820061660194184, 0.40636485712528843, 0.7021092632450649, 0.5681991151400977, 0.6804385459036598, 0.4609905643449374, 0.6272725296908814, 0.3652302434284041, 0.4677710297409992, 0.37180246905461555, 0.6271852731704426]
test_predictions = trainer.predict(dataset_test)

test_predictions

# Apply sigmoid to the logits to get probabilities
test_probabilities = torch.sigmoid(torch.tensor(test_predictions.predictions)).numpy()

# Apply the thresholds to get binary predictions
binary_predictions = np.zeros(test_probabilities.shape)
for i in range(len(classes)):  # Iterate over the classes
    binary_predictions[:, i] = (test_probabilities[:, i] >= optimized_thresholds[i]).astype(int)

# Extract the true labels
true_labels = test_predictions.label_ids

# Trainer evaluate
trainer.evaluate(dataset_test)

# Compute f1 metric
final_f1 = f1_score(true_labels, binary_predictions, average='macro')

print("\nFinal Evaluation:")
print(f"Macro F1 Score: {final_f1}")

with open('data/test_for_student.json', 'r', encoding='utf-8') as f:
    test_for_student = json.load(f)

def process_test_data(data):
    texts = []
    ids = []
    for id, info in data.items():
        title = clean_text(info['Title'])
        abstract = clean_text(info['Abstract'])
        text = f"{title} [SEP] {abstract}"
        texts.append(text)
        ids.append(id)
    return texts, ids

test_texts, test_ids = process_test_data(test_for_student)

# Tokenize the test data
inputs = tokenizer(test_texts, padding=True, truncation=True, return_tensors="pt")
inputs = {key: value.to(device) for key, value in inputs.items()}  # Move inputs to the same devic

# Make predictions using the model
model.to(device)
model.eval()
with torch.no_grad():
    logits = model(**inputs).logits

# Convert logits to probabilities using sigmoid function
sigmoid = torch.nn.Sigmoid()
test_probabilities = sigmoid(logits).cpu().numpy()

# Create a binary predictions array based on thresholds
binary_predictions = np.zeros(test_probabilities.shape)
for i in range(len(classes)):  # Iterate over each class
    binary_predictions[:, i] = (test_probabilities[:, i] >= optimized_thresholds[i]).astype(int)

output_predictions = []
for i, test_id in enumerate(test_ids):
    row = [test_id] + binary_predictions[i].tolist()
    output_predictions.append(row)

# Convert the output to a DataFrame for easy saving
columns = ["id"] + classes
submission_df = pd.DataFrame(output_predictions, columns=columns)

# Save to a CSV file (optional)
submission_df.to_csv("submission.csv", index=False)
print("Predictions saved to submission.csv")



# Save tokenizer
tokenizer.save_pretrained('./research_classification_bert')

# Save model
trainer.save_model('./research_classification_bert')

!zip -r research_classification_bert.zip research_classification_bert/