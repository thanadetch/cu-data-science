{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-06T08:16:50.809856Z",
     "start_time": "2024-10-06T08:16:01.424380Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "classes = ['CE', 'ENV', 'BME', 'PE', 'METAL', 'ME', 'EE', 'CPE', 'OPTIC', 'NANO', 'CHE',\n",
    "           'MATENG', 'AGRI', 'EDU', 'IE', 'SAFETY', 'MATH', 'MATSCI']\n",
    "\n",
    "# Load model and tokenizer from Hugging Face repository\n",
    "model_name = \"thanadetch/research_classification_bert\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Function to clean text (preprocessing)\n",
    "def clean_text(text):\n",
    "    # More comprehensive text cleaning\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Remove extra whitespace\n",
    "    text = re.sub(r'[^\\w\\s\\.\\-\\,]', '', text)  # Keep periods, hyphens, and commas\n",
    "    text = text.lower().strip()\n",
    "    return text\n",
    "\n",
    "\n",
    "# Sample test data (replace this with your actual test data)\n",
    "# In practice, this could be loaded from a file or provided through an API\n",
    "with open('data/test_for_student.json', 'r', encoding='utf-8') as f:\n",
    "    test_for_student = json.load(f)\n",
    "\n",
    "\n",
    "# Preprocess the test data (cleaning and preparing the text)\n",
    "def process_test_data(data):\n",
    "    texts = []\n",
    "    ids = []\n",
    "    for id, info in data.items():\n",
    "        title = clean_text(info['Title'])\n",
    "        abstract = clean_text(info['Abstract'])\n",
    "        text = f\"{title} [SEP] {abstract}\"\n",
    "        texts.append(text)\n",
    "        ids.append(id)\n",
    "    return texts, ids\n",
    "\n",
    "# Process the test data\n",
    "test_texts, test_ids = process_test_data(test_for_student)\n",
    "\n",
    "# Tokenize the test data for the model\n",
    "inputs = tokenizer(test_texts, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "inputs = {key: value.to(device) for key, value in inputs.items()} \n",
    "\n",
    "# Make predictions on the test data\n",
    "model.to(device)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    logits = model(**inputs).logits\n",
    "\n",
    "# Convert logits to probabilities using sigmoid function\n",
    "sigmoid = torch.nn.Sigmoid()\n",
    "test_probabilities = sigmoid(logits).cpu().numpy()\n",
    "\n",
    "optimized_thresholds = [0.5070643424987793, 0.5264949202537537, 0.5755283832550049, 0.48306921124458313, 0.5450738668441772, 0.5368289351463318, 0.5293850302696228, 0.5136227011680603, 0.43008750677108765, 0.6268097758293152, 0.4477621018886566, 0.6254380941390991, 0.4889925718307495, 0.6306849718093872, 0.5403478741645813, 0.5076555013656616, 0.580173671245575, 0.5481595396995544]\n",
    "\n",
    "# Create a binary predictions array based on thresholds\n",
    "binary_predictions = np.zeros(test_probabilities.shape)\n",
    "for i in range(len(classes)):  # Iterate over each class\n",
    "    binary_predictions[:, i] = (test_probabilities[:, i] >= optimized_thresholds[i]).astype(int)\n",
    "\n",
    "output_predictions = []\n",
    "for i, test_id in enumerate(test_ids):\n",
    "    row = [test_id] + binary_predictions[i].tolist()\n",
    "    output_predictions.append(row)\n",
    "\n",
    "# Convert the output to a DataFrame for easy saving\n",
    "columns = [\"id\"] + classes\n",
    "submission_df = pd.DataFrame(output_predictions, columns=columns)\n",
    "\n",
    "# Save to a CSV file (optional)\n",
    "submission_df.to_csv(\"submission.csv\", index=False)\n",
    "print(\"Predictions saved to submission.csv\")"
   ],
   "id": "4968216f52ba097b",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "28dcbe92b55b4c58a997a7782f0ee47f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/712k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e100c51baebd4eccada7065fcc2b775f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions saved to submission.csv\n"
     ]
    }
   ],
   "execution_count": 2
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
